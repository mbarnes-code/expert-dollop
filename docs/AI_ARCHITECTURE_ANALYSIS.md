# AI Services Architecture Analysis

**Date:** 2025-12-03  
**Purpose:** Evaluate architecture options for AI capabilities in the monorepo

## Context

The monorepo currently has:
- **Existing AI services** in `apps/ai/`: n8n, firecrawl, goose (full applications)
- **New consolidated services**: models, chat, analytics (extracted capabilities)
- **Newly discovered AI**: Dispatch (incident management), SecurityOnion (threat detection)
- **Future integration**: HELK (Hunting ELK with ML capabilities)

## The Architectural Question

**Should the AI capabilities be:**
1. **Broken down further** into granular shared services that the whole monorepo can use?
2. **Kept in their current services** and consumed by other services as-is?

---

## Current State Assessment

### Option 1: Current Structure (Monolithic Services)

```
apps/ai/
â”œâ”€â”€ n8n/              # Full n8n application (200+ LangChain nodes)
â”œâ”€â”€ firecrawl-api/    # Full Firecrawl service (AI web scraping)
â”œâ”€â”€ goose/            # Full Goose agent runtime
â”œâ”€â”€ models/           # Extracted: Model registry UI/API
â”œâ”€â”€ chat/             # Extracted: Unified chat interface
â””â”€â”€ analytics/        # Extracted: Usage tracking
```

**Pros:**
- âœ… Services are self-contained and independently deployable
- âœ… No breaking changes to existing applications
- âœ… Each service owns its complete feature set
- âœ… Clear boundaries and ownership

**Cons:**
- âŒ Code duplication across services (already identified)
- âŒ Other monorepo services can't easily use AI capabilities
- âŒ No shared AI infrastructure
- âŒ Difficult to compose AI features across services

### Option 2: Granular Shared Services (SOA Architecture)

```
apps/ai/
â”œâ”€â”€ n8n/              # Full n8n application (uses libs)
â”œâ”€â”€ firecrawl-api/    # Full Firecrawl service (uses libs)
â”œâ”€â”€ goose/            # Full Goose agent runtime (uses libs)
â”œâ”€â”€ models/           # Model registry UI/API
â”œâ”€â”€ chat/             # Chat interface UI/API
â”œâ”€â”€ analytics/        # Analytics dashboard UI/API
â”œâ”€â”€ vector-store/     # NEW: Vector DB service
â”œâ”€â”€ embeddings/       # NEW: Embedding generation service
â”œâ”€â”€ extraction/       # NEW: LLM extraction service
â”œâ”€â”€ agents/           # NEW: Agent orchestration service
â”œâ”€â”€ prompts/          # NEW: Prompt management service
â””â”€â”€ evaluation/       # NEW: AI quality assessment service

libs/ai/
â”œâ”€â”€ model-registry/         # Already created
â”œâ”€â”€ error-handling/         # Already created
â”œâ”€â”€ next-app-base/         # Already created
â”œâ”€â”€ llm-clients/           # NEW: LLM provider abstractions
â”œâ”€â”€ vector-db-clients/     # NEW: Vector DB clients
â”œâ”€â”€ langchain-core/        # NEW: LangChain utilities
â”œâ”€â”€ prompt-templates/      # NEW: Shared prompt library
â”œâ”€â”€ token-counter/         # NEW: Token counting utilities
â””â”€â”€ ai-middleware/         # NEW: Common AI middleware
```

**Pros:**
- âœ… Granular, reusable AI capabilities
- âœ… Any monorepo service can use AI features
- âœ… Single source of truth for each capability
- âœ… Easier to test individual components
- âœ… Better code reuse across the monorepo

**Cons:**
- âŒ More complex architecture
- âŒ More services to maintain
- âŒ Potential performance overhead (network calls)
- âŒ Requires significant refactoring of existing services

---

## Recommendation: Hybrid Approach

**Combine both strategies for maximum flexibility:**

### 1. Keep Full Applications As-Is
```
apps/ai/
â”œâ”€â”€ n8n/              # Keep as full application
â”œâ”€â”€ firecrawl-api/    # Keep as full application  
â”œâ”€â”€ goose/            # Keep as full application
```

**Rationale:**
- These are complete, production-ready applications
- Users may want the full n8n/Firecrawl/Goose experience
- Breaking them down would lose cohesion
- They can consume shared libraries without refactoring

### 2. Create Shared Library Layer
```
libs/ai/
â”œâ”€â”€ model-registry/         âœ… Already exists
â”œâ”€â”€ error-handling/         âœ… Already exists
â”œâ”€â”€ next-app-base/         âœ… Already exists
â”œâ”€â”€ llm-clients/           ğŸ†• LLM provider abstractions
â”œâ”€â”€ vector-db-clients/     ğŸ†• Vector DB clients
â”œâ”€â”€ prompt-manager/        ğŸ†• Prompt storage & versioning
â”œâ”€â”€ token-utils/           ğŸ†• Token counting & cost calc
â”œâ”€â”€ langchain-utils/       ğŸ†• LangChain helpers
â””â”€â”€ embeddings-client/     ğŸ†• Embedding generation
```

**Rationale:**
- Libraries are consumed by both apps and services
- Zero deployment overhead (just imports)
- Easy to test and version
- Can be used by any service in the monorepo

### 3. Create Focused Microservices
```
apps/ai/
â”œâ”€â”€ models/           âœ… Model registry UI/API (exists)
â”œâ”€â”€ chat/             âœ… Chat interface (exists)
â”œâ”€â”€ analytics/        âœ… Usage tracking (exists)
â”œâ”€â”€ vector-store/     ğŸ†• Vector operations as a service
â”œâ”€â”€ extraction/       ğŸ†• LLM extraction as a service
â”œâ”€â”€ prompts/          ğŸ†• Prompt management service
â””â”€â”€ agents/           ğŸ†• Agent orchestration service
```

**Rationale:**
- These require state management (databases, caching)
- Better as services than libraries
- Can be independently scaled
- Provide REST APIs for easy consumption

---

## Integration with Newly Discovered AI

### Dispatch AI (Incident Management)

**Current Location:** `features/dispatch/src/dispatch/ai/`

**Capabilities:**
- Incident summarization
- Tag recommendations
- Tactical report generation
- Prompt management system

**Recommendation:**
```
Option A: Extract to apps/ai/incident-management/
- Create dedicated service for incident AI
- Integrate with SecurityOnion for security incidents
- Use shared libs/ai/prompt-manager

Option B: Extract prompts to libs/ai/prompt-manager/
- Keep Dispatch's AI logic in Dispatch
- Share prompt templates across monorepo
- Add incident-specific prompts
```

**Best Choice:** Option B (extract prompts, keep logic in Dispatch)
- Dispatch-specific AI should stay in Dispatch
- Share the valuable prompt templates
- Less disruption to existing Dispatch service

### SecurityOnion AI (Threat Detection)

**Current Location:** `features/securityonion/salt/sensoroni/files/analyzers/`

**Capabilities:**
- EmailRep analyzer (reputation checking)
- Elasticsearch analyzer (query building)
- Threat intelligence integrations

**Recommendation:**
```
Option A: Create apps/ai/security-analysis/
- Centralize all security AI
- Integrate with HELK when ready
- Build security-specific AI pipelines

Option B: Keep in SecurityOnion, extract common utilities
- Security logic stays with security tools
- Extract reusable components to libs/ai/
```

**Best Choice:** Option B (keep in SecurityOnion)
- These analyzers are tightly coupled to SecurityOnion
- Not general-purpose AI capabilities
- Would complicate the security service to split

### HELK Integration (Future)

**Project:** `features/HELK/` - Hunting ELK with ML capabilities

**Capabilities:**
- Apache Spark for big data analytics
- Jupyter notebooks for data science
- GraphFrames for graph analysis
- Machine learning pipelines
- Elasticsearch-based threat hunting

**Recommendation:**
```
Create: apps/ai/threat-hunting/
- Dedicated service for threat hunting AI
- Integrates HELK's ML capabilities
- Uses libs/ai/vector-db-clients for Elasticsearch
- Jupyter notebooks for interactive analysis
- Connects to SecurityOnion data sources
```

**Implementation Priority:** Medium-High
- HELK provides unique ML capabilities not found elsewhere
- Perfect for analyzing SecurityOnion data
- Natural fit with vector-store service
- Wait until SecurityOnion integration is more mature

---

## Proposed Architecture (Hybrid)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Monorepo Services                        â”‚
â”‚  (Any service can import libs/ai/*)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–²
                            â”‚ imports
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Shared AI Libraries                      â”‚
â”‚  libs/ai/                                                   â”‚
â”‚  â”œâ”€â”€ model-registry/      (model info, pricing)            â”‚
â”‚  â”œâ”€â”€ llm-clients/         (OpenAI, Anthropic, etc.)        â”‚
â”‚  â”œâ”€â”€ vector-db-clients/   (Pinecone, Qdrant, etc.)        â”‚
â”‚  â”œâ”€â”€ prompt-manager/      (templates, versioning)          â”‚
â”‚  â”œâ”€â”€ token-utils/         (counting, cost calc)            â”‚
â”‚  â””â”€â”€ embeddings-client/   (textâ†’vectors)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–²
                            â”‚ uses
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              AI Microservices (REST APIs)                   â”‚
â”‚  apps/ai/                                                   â”‚
â”‚  â”œâ”€â”€ models/         (UI + API for model registry)         â”‚
â”‚  â”œâ”€â”€ chat/           (UI + API for chat)                   â”‚
â”‚  â”œâ”€â”€ analytics/      (UI + API for usage tracking)         â”‚
â”‚  â”œâ”€â”€ vector-store/   (NEW: Vector operations)              â”‚
â”‚  â”œâ”€â”€ extraction/     (NEW: LLM extraction)                 â”‚
â”‚  â”œâ”€â”€ prompts/        (NEW: Prompt management)              â”‚
â”‚  â””â”€â”€ agents/         (NEW: Agent orchestration)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–²
                            â”‚ uses
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Full AI Applications (Self-Contained)             â”‚
â”‚  apps/ai/                                                   â”‚
â”‚  â”œâ”€â”€ n8n/            (Full n8n - 200+ nodes)               â”‚
â”‚  â”œâ”€â”€ firecrawl-api/  (Full Firecrawl)                     â”‚
â”‚  â”œâ”€â”€ goose/          (Full Goose agent runtime)            â”‚
â”‚  â””â”€â”€ dispatch/       (Incident management - stays in own)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Implementation Roadmap

### Phase 1: Library Extraction (Low Risk, High Value)
**Timeline:** 1-2 weeks

1. âœ… `libs/ai/model-registry` - Already done
2. âœ… `libs/ai/error-handling` - Already done
3. âœ… `libs/ai/next-app-base` - Already done
4. ğŸ†• `libs/ai/prompt-manager` - Extract from Dispatch
5. ğŸ†• `libs/ai/token-utils` - Extract from Goose
6. ğŸ†• `libs/ai/llm-clients` - Extract common patterns

**Benefit:** Immediate code reuse, zero deployment complexity

### Phase 2: Focused Services (Medium Risk, High Value)
**Timeline:** 4-6 weeks

1. âœ… `apps/ai/models` - Already done
2. âœ… `apps/ai/chat` - Already done
3. âœ… `apps/ai/analytics` - Already done
4. ğŸ†• `apps/ai/vector-store` - RAG infrastructure
5. ğŸ†• `apps/ai/prompts` - Prompt management UI

**Benefit:** Reusable services, independent scaling

### Phase 3: Advanced Capabilities (High Risk, High Value)
**Timeline:** 8-12 weeks

1. ğŸ†• `apps/ai/extraction` - LLM extraction service
2. ğŸ†• `apps/ai/agents` - Agent orchestration
3. ğŸ†• `apps/ai/threat-hunting` - HELK integration
4. ğŸ†• `apps/ai/embeddings` - Embedding service

**Benefit:** Advanced AI capabilities across monorepo

---

## Decision Matrix

| Capability | Keep in App | Extract to Service | Extract to Library | Recommendation |
|------------|-------------|-------------------|-------------------|----------------|
| n8n LangChain nodes | âœ… | âŒ | Partial | Keep + share libs |
| Firecrawl scraping | âœ… | âŒ | Partial | Keep + share libs |
| Goose agents | âœ… | âŒ | Partial | Keep + share libs |
| Model registry | âŒ | âœ… | âœ… | Both (service + lib) |
| Chat interface | âŒ | âœ… | Partial | Service + lib helpers |
| Analytics | âŒ | âœ… | Partial | Service + lib utils |
| Vector operations | âŒ | âœ… | âœ… | Both (service + lib) |
| LLM extraction | âŒ | âœ… | Partial | Service + lib clients |
| Prompt management | âŒ | âœ… | âœ… | Both (service + lib) |
| Token counting | âŒ | âŒ | âœ… | Library only |
| Embeddings | âŒ | âœ… | âœ… | Both (service + lib) |
| Dispatch incident AI | âœ… | âŒ | Prompts only | Keep in Dispatch |
| SecurityOnion analyzers | âœ… | âŒ | Utils only | Keep in SecurityOnion |
| HELK ML pipelines | âŒ | âœ… | Partial | New threat-hunting service |

---

## Answers to User's Questions

### Q: Should services be broken down further?
**A: Selectively yes, using a hybrid approach:**
- Keep n8n, Firecrawl, Goose as full applications
- Extract reusable components to shared libraries
- Create focused microservices for stateful operations
- Don't break down domain-specific AI (Dispatch, SecurityOnion)

### Q: Can services provide value to monorepo in current form?
**A: Yes, but limited:**
- Current services (models, chat, analytics) provide value as-is
- Full apps (n8n, Firecrawl, Goose) are useful standalone
- However, without shared libraries, code duplication remains
- Other monorepo services can't easily consume AI capabilities

### Q: What about Dispatch and SecurityOnion AI?
**A: Keep domain-specific, share primitives:**
- **Dispatch:** Extract prompt templates to `libs/ai/prompt-manager`, keep incident AI in Dispatch
- **SecurityOnion:** Extract common utilities to libs, keep analyzers in SecurityOnion
- Both can use shared AI libraries (models, LLM clients, etc.)

### Q: How does HELK fit in?
**A: Create dedicated threat-hunting service:**
- HELK provides unique ML capabilities (Spark, Jupyter, GraphFrames)
- Natural integration with SecurityOnion data
- Can leverage shared vector-store service for Elasticsearch
- Medium-high priority after core infrastructure stabilizes

---

## Recommendation Summary

**Adopt the Hybrid Architecture:**

1. **Keep full applications** (n8n, Firecrawl, Goose) as-is
2. **Create shared libraries** for common primitives (prompts, tokens, LLM clients, vector clients)
3. **Build focused microservices** for stateful operations (vector-store, extraction, agents)
4. **Leave domain-specific AI** in their respective services (Dispatch incident AI, SecurityOnion analyzers)
5. **Plan for HELK integration** as a dedicated threat-hunting service

**This approach:**
- âœ… Maximizes code reuse without breaking existing apps
- âœ… Enables monorepo-wide AI consumption
- âœ… Maintains clear boundaries and ownership
- âœ… Scales with the monorepo's needs
- âœ… Minimizes deployment complexity

---

**Next Steps:**
1. Review and approve hybrid architecture approach
2. Prioritize Phase 1 library extraction
3. Plan Phase 2 microservices implementation
4. Define integration points for Dispatch/SecurityOnion
5. Schedule HELK integration planning
